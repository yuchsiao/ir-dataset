{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74b5832b-a439-49e6-98f3-fe9ea501b602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from beir import LoggingHandler\n",
    "from beir import util\n",
    "from beir.datasets.data_loader import GenericDataLoader\n",
    "from beir.retrieval.search.dense import DenseRetrievalExactSearch as DRES\n",
    "from beir.retrieval.evaluation import EvaluateRetrieval\n",
    "from beir.retrieval import models\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import dataclasses\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb891a86-df68-48ae-9c89-757183886381",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class BeirTopKSimilarityMetadata:\n",
    "    dataset_name: str\n",
    "    num_queries: int\n",
    "    num_documents: int\n",
    "    num_positive_annotations: int\n",
    "    model_name: str\n",
    "    top_k: bool\n",
    "    include_text: bool\n",
    "\n",
    "    def __init__(self, dataset_name, queries, corpus, qrels, model_name, top_k, include_text):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.num_queries = len(queries)\n",
    "        self.num_documents = len(corpus)\n",
    "        self.num_positive_annotations = sum(len(rel_docids) for qid, rel_docids in qrels.items())\n",
    "        self.model_name = model_name\n",
    "        self.top_k = top_k\n",
    "        self.include_text = include_text\n",
    "        \n",
    "\n",
    "def load_data(dataset_name, datasets_path=\"..\"):\n",
    "    url = \"https://public.ukp.informatik.tu-darmstadt.de/thakur/BEIR/datasets/{}.zip\".format(dataset_name)\n",
    "    out_dir = os.path.join(\"..\", \"datasets\")\n",
    "    data_path = util.download_and_unzip(url, out_dir)\n",
    "    \n",
    "    #### Provide the data_path where scifact has been downloaded and unzipped\n",
    "    corpus, queries, qrels = GenericDataLoader(data_folder=data_path).load(split=\"test\")\n",
    "    return data_path, corpus, queries, qrels\n",
    "\n",
    "\n",
    "def compute_similarity(model_name, batch_size, score_function=None) -> dict[str, dict[str, float]]:\n",
    "    if score_function is None:\n",
    "        score_function = \"dot\"\n",
    "    if score_function not in (\"dot\", \"cos_sim\"):\n",
    "        raise ValueError(f\"score_function must be either 'dot' or 'cos_sim'. Received: '{score_function}'\")\n",
    "\n",
    "    model = DRES(models.SentenceBERT(model_name, trust_remote_code=True), batch_size=batch_size, trust_remote_code=True)\n",
    "    retriever = EvaluateRetrieval(model, score_function=score_function)\n",
    "    q_doc_sim = retriever.retrieve(corpus, queries)\n",
    "    return q_doc_sim\n",
    "\n",
    "\n",
    "def get_top_k_similar_docs(doc_sim: dict[str, float], k: int) -> dict[str, float]:\n",
    "    return dict(sorted(doc_sim.items(), key=lambda x: x[1], reverse=True)[:k])\n",
    "\n",
    "\n",
    "def get_labels_similarities(query, top_k_similar_docs, qrels):\n",
    "    labels = {}\n",
    "    similarity_scores = {}\n",
    "    for doc, similarity_score in top_k_similar_docs.items():\n",
    "        if doc in qrels[query] and qrels[query][doc] == 1:\n",
    "            labels[doc] = 1\n",
    "        else:\n",
    "            labels[doc] = 0\n",
    "        similarity_scores[doc] = similarity_score\n",
    "    return labels, similarity_scores\n",
    "        \n",
    "\n",
    "def get_top_k_data_frame(q_doc_sim, top_k, include_text=False):\n",
    "    qids = []\n",
    "    docids = []\n",
    "    rels = []\n",
    "    sims = []\n",
    "    if include_text:\n",
    "        q_texts = []\n",
    "        title_texts = []\n",
    "        doc_texts = []\n",
    "    \n",
    "    for qid, doc_sim in q_doc_sim.items():\n",
    "        top_k_doc_scores = get_top_k_similar_docs(q_doc_sim[qid], top_k)\n",
    "        docid_labels, similarities = get_labels_similarities(qid, top_k_doc_scores, qrels)\n",
    "        for docid, rel in docid_labels.items():\n",
    "            qids.append(qid)\n",
    "            docids.append(docid)\n",
    "            rels.append(rel)\n",
    "            sims.append(similarities[docid])\n",
    "            if include_text:\n",
    "                q_texts.append(queries[qid])\n",
    "                title_texts.append(corpus[docid].get(\"title\", \"\"))\n",
    "                doc_texts.append(corpus[docid].get(\"text\", \"\"))\n",
    "    data = {\n",
    "        \"qid\": qids, \n",
    "        \"docid\": docids, \n",
    "        \"rel\": rels,\n",
    "        \"sim\": sims,\n",
    "    }\n",
    "    \n",
    "    if include_text:\n",
    "        data.update({\n",
    "            \"query\": q_texts,\n",
    "            \"title\": title_texts,\n",
    "            \"corpus\": doc_texts,\n",
    "        })\n",
    "    return pd.DataFrame(data)    \n",
    "\n",
    "\n",
    "def main(dataset_name=None):\n",
    "    datasets = [\n",
    "        \"trec-covid\",\n",
    "        \"nq\",\n",
    "        \"hotpotqa\",\n",
    "        \"arguana\",\n",
    "        \"webis-touche2020\",\n",
    "        \"cqadupstack\",\n",
    "        \"dbpedia-entity\",\n",
    "        \"scidocs\",\n",
    "        \"fever\",\n",
    "    ]\n",
    "    # dataset_name = \"scifact\"\n",
    "    model_name = \"msmarco-distilbert-base-tas-b\"\n",
    "    batch_size = 64\n",
    "    top_k = 500\n",
    "    include_text = False\n",
    "\n",
    "    def process_dataset(dataset_name):\n",
    "        # Load dataset.\n",
    "        logging.info(\"## Load dataset.\")\n",
    "        data_path, corpus, queries, qrels = load_data(dataset_name)\n",
    "        # Compute similarity scores.\n",
    "        logging.info(\"## Compute similarity scores.\")\n",
    "        q_doc_sim = compute_similarity(model_name, batch_size)    \n",
    "        # Get DF.\n",
    "        logging.info(\"## Get the dataframe.\")\n",
    "        df = get_top_k_data_frame(q_doc_sim, top_k, include_text)\n",
    "        metadata = BeirTopKSimilarityMetadata(\n",
    "            dataset_name, queries, corpus, qrels, model_name, top_k, include_text)\n",
    "\n",
    "        # Write to files\n",
    "        logging.info(\"## Write to files.\")\n",
    "        df.to_csv(data_path + \".csv\")\n",
    "        with open(data_path + \"_metadata.json\", \"w\") as fout:\n",
    "            json.dump(dataclasses.asdict(metadata), fout, indent=4)\n",
    "\n",
    "    \n",
    "    if dataset_name is None:\n",
    "        for dataset_idx, dataset_name in enumerate(datasets):\n",
    "            logging.info(f\"#### DATASET {dataset_idx}: {dataset_name}\")\n",
    "            process_dataset(dataset_idx, dataset_name)\n",
    "    else:\n",
    "        # Only process the specified dataset.\n",
    "        logging.info(f\"#### DATASET: {dataset_name}\")\n",
    "        process_dataset(dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee7ce647-911d-4b1e-82dc-eeb144de4f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:12:49 - #### DATASET: scifact\n",
      "2024-09-02 16:12:49 - ## Load dataset.\n",
      "2024-09-02 16:12:49 - Loading Corpus...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45cd831646a47c4acae85eddd885c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:12:49 - Loaded 5183 TEST Documents.\n",
      "2024-09-02 16:12:49 - Doc Example: {'text': 'Alterations of the architecture of cerebral white matter in the developing human brain can affect cortical development and result in functional disabilities. A line scan diffusion-weighted magnetic resonance imaging (MRI) sequence with diffusion tensor analysis was applied to measure the apparent diffusion coefficient, to calculate relative anisotropy, and to delineate three-dimensional fiber architecture in cerebral white matter in preterm (n = 17) and full-term infants (n = 7). To assess effects of prematurity on cerebral white matter development, early gestation preterm infants (n = 10) were studied a second time at term. In the central white matter the mean apparent diffusion coefficient at 28 wk was high, 1.8 microm2/ms, and decreased toward term to 1.2 microm2/ms. In the posterior limb of the internal capsule, the mean apparent diffusion coefficients at both times were similar (1.2 versus 1.1 microm2/ms). Relative anisotropy was higher the closer birth was to term with greater absolute values in the internal capsule than in the central white matter. Preterm infants at term showed higher mean diffusion coefficients in the central white matter (1.4 +/- 0.24 versus 1.15 +/- 0.09 microm2/ms, p = 0.016) and lower relative anisotropy in both areas compared with full-term infants (white matter, 10.9 +/- 0.6 versus 22.9 +/- 3.0%, p = 0.001; internal capsule, 24.0 +/- 4.44 versus 33.1 +/- 0.6% p = 0.006). Nonmyelinated fibers in the corpus callosum were visible by diffusion tensor MRI as early as 28 wk; full-term and preterm infants at term showed marked differences in white matter fiber organization. The data indicate that quantitative assessment of water diffusion by diffusion tensor MRI provides insight into microstructural development in cerebral white matter in living infants.', 'title': 'Microstructural development of human newborn cerebral white matter assessed in vivo by diffusion tensor magnetic resonance imaging.'}\n",
      "2024-09-02 16:12:49 - Loading Queries...\n",
      "2024-09-02 16:12:49 - Loaded 300 TEST Queries.\n",
      "2024-09-02 16:12:49 - Query Example: 0-dimensional biomaterials show inductive properties.\n",
      "2024-09-02 16:12:49 - ## Compute similarity scores.\n",
      "2024-09-02 16:12:49 - Use pytorch device_name: cuda\n",
      "2024-09-02 16:12:49 - Load pretrained SentenceTransformer: msmarco-distilbert-base-tas-b\n",
      "2024-09-02 16:12:50 - Encoding Queries...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f4ead41f714eb7af3ddbe4ff0fea04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:12:50 - Sorting Corpus by document length (Longest first)...\n",
      "2024-09-02 16:12:50 - Encoding Corpus in batches... Warning: This might take a while!\n",
      "2024-09-02 16:12:50 - Scoring Function: Dot Product (dot)\n",
      "2024-09-02 16:12:50 - Encoding Batch 1/1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f92b2ca6354a188bbb6414e1ac41c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/81 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-09-02 16:13:05 - ## Get the dataframe.\n",
      "2024-09-02 16:13:05 - ## Write to files.\n"
     ]
    }
   ],
   "source": [
    "main(dataset_name=\"scifact\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
